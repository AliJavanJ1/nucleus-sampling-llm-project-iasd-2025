{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ecb47ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/VSCodeProjects/iasd/llm-project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math, random, re, json, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "from generator import LLMEngine, ModelConfig, DecodeConfig\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346980c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SEED = 44\n",
    "MAX_NEW_TOKENS = 200\n",
    "\n",
    "MODEL_ID = \"gpt2-large\"\n",
    "# MODEL_ID = \"Qwen/Qwen3-1.7B-Base\"\n",
    "\n",
    "SWEEP_T = [0.5, 0.7, 0.9, 1.0, 1.2]\n",
    "SWEEP_K = [1, 10, 40, 160, 640]\n",
    "SWEEP_P = [0.85, 0.90, 0.95, 0.97, 0.99]\n",
    "SWEEP_BEAMS = [1, 2, 4, 8, 16, 32, 64]\n",
    "\n",
    "N_PROMPTS_GEN = 120      # reduce to fit 6GB comfortably\n",
    "N_PROMPTS_PPL = 40       # smaller because scoring is expensive\n",
    "\n",
    "MICROBATCH = 20          # key VRAM control: number of prompts per generate() call\n",
    "\n",
    "def seed_everything(seed=GLOBAL_SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e71e8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,\n",
       " 40,\n",
       " 'NuCoal Resources have suffered another blow after their case against the')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPTS_PATH = \"prompts_openwebtext_subset.json\"\n",
    "\n",
    "gpt2_tok = AutoTokenizer.from_pretrained(\"gpt2-large\", use_fast=True)\n",
    "\n",
    "def first_paragraph(text: str) -> str:\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    parts = re.split(r\"\\n\\s*\\n\", text)\n",
    "    para = parts[0].strip() if parts else \"\"\n",
    "    return para if para else text.split(\"\\n\")[0].strip()\n",
    "\n",
    "def make_prompt_from_para(para: str, min_tok=1, max_tok=40) -> str:\n",
    "    ids = gpt2_tok(para, add_special_tokens=False).input_ids\n",
    "    if len(ids) < min_tok:\n",
    "        return \"\"\n",
    "    L = random.randint(min_tok, min(max_tok, len(ids)))\n",
    "    return gpt2_tok.decode(ids[:L])\n",
    "\n",
    "if os.path.exists(PROMPTS_PATH):\n",
    "    with open(PROMPTS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        prompts = json.load(f)\n",
    "else:\n",
    "    ds = load_dataset(\n",
    "        \"dylanebert/openwebtext\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        revision=\"refs/convert/parquet\",\n",
    "    )\n",
    "    ds = ds.shuffle(seed=GLOBAL_SEED, buffer_size=50_000)\n",
    "\n",
    "    docs = [ex[\"text\"] for ex in ds.take(1200)]\n",
    "    prompts = []\n",
    "    for txt in docs:\n",
    "        pr = make_prompt_from_para(first_paragraph(txt), 1, 40)\n",
    "        if pr:\n",
    "            prompts.append(pr)\n",
    "        if len(prompts) >= max(N_PROMPTS_GEN, N_PROMPTS_PPL):\n",
    "            break\n",
    "\n",
    "    with open(PROMPTS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(prompts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "prompts_gen = prompts[:N_PROMPTS_GEN]\n",
    "prompts_ppl = prompts[:N_PROMPTS_PPL]\n",
    "\n",
    "len(prompts_gen), len(prompts_ppl), prompts_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e44ab08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_engine(model_id: str) -> LLMEngine:\n",
    "    return LLMEngine(ModelConfig(model_id=model_id))\n",
    "\n",
    "def dcfg(method: str, t=1.0, k=0, p=1.0, num_beams=1, seed=GLOBAL_SEED) -> DecodeConfig:\n",
    "    return DecodeConfig(\n",
    "        method=method,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        t=t,\n",
    "        k=k,\n",
    "        p=p,\n",
    "        num_beams=num_beams,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "engine = make_engine(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb361e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_microbatched(text_list, decode_cfg, microbatch=MICROBATCH):\n",
    "    outs = []\n",
    "    for i in range(0, len(text_list), microbatch):\n",
    "        chunk = text_list[i:i+microbatch]\n",
    "        chunk_out = engine.generate(text_list=chunk, dcfg=decode_cfg)\n",
    "        outs.extend(list(chunk_out))\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()  # helps with fragmentation on small GPUs\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98ba7076",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = BLEU(effective_order=True)\n",
    "\n",
    "def word_tokens(s: str):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", (s or \"\").lower(), flags=re.UNICODE)\n",
    "\n",
    "def has_repeated_suffix(text: str, min_ng=2, max_ng=20, min_rep=3) -> bool:\n",
    "    toks = word_tokens(text)\n",
    "    L = len(toks)\n",
    "    if L < min_ng * min_rep:\n",
    "        return False\n",
    "    max_ng = min(max_ng, L // min_rep)\n",
    "    for n in range(min_ng, max_ng + 1):\n",
    "        unit = toks[-n:]\n",
    "        for r in range(min_rep, L // n + 1):\n",
    "            if toks[-n*r:] == unit * r:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def distinct_trigrams(text: str) -> int:\n",
    "    toks = word_tokens(text)\n",
    "    if len(toks) < 3:\n",
    "        return 0\n",
    "    return len({tuple(toks[i:i+3]) for i in range(len(toks)-2)})\n",
    "\n",
    "def approx_self_bleu(texts, n_eval=120, n_refs=30, seed=GLOBAL_SEED) -> float:\n",
    "    rng = random.Random(seed)\n",
    "    N = len(texts)\n",
    "    idxs = list(range(N))\n",
    "    rng.shuffle(idxs)\n",
    "    idxs = idxs[:min(n_eval, N)]\n",
    "    scores = []\n",
    "    for i in idxs:\n",
    "        hyp = texts[i]\n",
    "        pool = [texts[j] for j in range(N) if j != i]\n",
    "        if not pool:\n",
    "            continue\n",
    "        refs = pool if len(pool) <= n_refs else rng.sample(pool, n_refs)\n",
    "        scores.append(bleu.sentence_score(hyp, refs).score / 100.0)\n",
    "    return float(np.mean(scores)) if scores else float(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e540feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, trust_remote_code=True)\n",
    "if ppl_tok.pad_token is None:\n",
    "    ppl_tok.pad_token = ppl_tok.eos_token\n",
    "\n",
    "ppl_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.float32,\n",
    "    device_map=None,\n",
    "    trust_remote_code=True,\n",
    ").to(\"cpu\").eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def continuation_ppl(prompt: str, full_text: str) -> float:\n",
    "    p_ids = ppl_tok(prompt, return_tensors=\"pt\").input_ids[0]\n",
    "    x_ids = ppl_tok(full_text, return_tensors=\"pt\").input_ids[0]\n",
    "    if len(x_ids) <= len(p_ids):\n",
    "        return float(\"inf\")\n",
    "\n",
    "    ctx_len = len(p_ids)\n",
    "    inp = x_ids.unsqueeze(0)\n",
    "\n",
    "    logits = ppl_model(inp).logits\n",
    "    logp = torch.log_softmax(logits[:, :-1, :], dim=-1)\n",
    "    tgt = inp[:, 1:]\n",
    "\n",
    "    start = max(ctx_len - 1, 0)\n",
    "    lp = logp[:, start:, :].gather(-1, tgt[:, start:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    nll = -lp.mean().item()\n",
    "    return math.exp(nll) if nll < 50 else float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70237eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 200/200 [00:37<00:00,  5.32tok/s]\n",
      "Generating: 100%|██████████| 200/200 [00:39<00:00,  5.10tok/s]\n",
      "Generating: 100%|██████████| 200/200 [00:33<00:00,  5.90tok/s]\n"
     ]
    }
   ],
   "source": [
    "def run_family(family: str, xs: list):\n",
    "    rows = []\n",
    "    for x in tqdm(xs, desc=f\"{MODEL_ID} {family}\"):\n",
    "        if family == \"temperature\":\n",
    "            cfg = dcfg(\"temperature\", t=float(x))\n",
    "        elif family == \"top_k\":\n",
    "            cfg = dcfg(\"top_k\", k=int(x))\n",
    "        elif family == \"top_p\":\n",
    "            cfg = dcfg(\"top_p\", p=float(x))\n",
    "        else:\n",
    "            raise ValueError(family)\n",
    "\n",
    "        texts = generate_microbatched(prompts_gen, cfg, microbatch=MICROBATCH)\n",
    "\n",
    "        repeat_rate = float(np.mean([has_repeated_suffix(t) for t in texts]))\n",
    "        self_bleu = approx_self_bleu(texts)\n",
    "\n",
    "        texts_ppl = texts[:len(prompts_ppl)]\n",
    "        ppl = float(np.mean([continuation_ppl(p, t) for p, t in zip(prompts_ppl, texts_ppl)]))\n",
    "\n",
    "        rows.append({\"model_id\": MODEL_ID, \"family\": family, \"x\": x,\n",
    "                     \"ppl\": ppl, \"repeat_rate\": repeat_rate, \"self_bleu\": self_bleu})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_main = pd.concat([\n",
    "    run_family(\"temperature\", SWEEP_T),\n",
    "    run_family(\"top_k\", SWEEP_K),\n",
    "    run_family(\"top_p\", SWEEP_P),\n",
    "], ignore_index=True)\n",
    "\n",
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ebe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(df, family, y, xlab, ylab, title):\n",
    "    sub = df[df.family == family].sort_values(\"x\")\n",
    "    plt.figure()\n",
    "    plt.plot(sub[\"x\"], sub[y], marker=\"o\")\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_metric(df_main, \"temperature\", \"ppl\", \"temperature (t)\", \"perplexity\", f\"{MODEL_ID}: PPL vs temperature\")\n",
    "plot_metric(df_main, \"top_k\", \"ppl\", \"top-k (k)\", \"perplexity\", f\"{MODEL_ID}: PPL vs top-k\")\n",
    "plot_metric(df_main, \"top_p\", \"ppl\", \"top-p (p)\", \"perplexity\", f\"{MODEL_ID}: PPL vs top-p\")\n",
    "\n",
    "plot_metric(df_main, \"temperature\", \"repeat_rate\", \"temperature (t)\", \"repetition rate\", f\"{MODEL_ID}: repetition vs temperature\")\n",
    "plot_metric(df_main, \"top_k\", \"repeat_rate\", \"top-k (k)\", \"repetition rate\", f\"{MODEL_ID}: repetition vs top-k\")\n",
    "plot_metric(df_main, \"top_p\", \"repeat_rate\", \"top-p (p)\", \"repetition rate\", f\"{MODEL_ID}: repetition vs top-p\")\n",
    "\n",
    "plot_metric(df_main, \"temperature\", \"self_bleu\", \"temperature (t)\", \"Self-BLEU4\", f\"{MODEL_ID}: Self-BLEU vs temperature\")\n",
    "plot_metric(df_main, \"top_k\", \"self_bleu\", \"top-k (k)\", \"Self-BLEU4\", f\"{MODEL_ID}: Self-BLEU vs top-k\")\n",
    "plot_metric(df_main, \"top_p\", \"self_bleu\", \"top-p (p)\", \"Self-BLEU4\", f\"{MODEL_ID}: Self-BLEU vs top-p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, trust_remote_code=True)\n",
    "\n",
    "def tok_len(text: str) -> int:\n",
    "    return len(len_tok(text, add_special_tokens=False).input_ids)\n",
    "\n",
    "STOP_EARLY_FRAC = 0.2\n",
    "rows = []\n",
    "\n",
    "for b in tqdm(SWEEP_BEAMS, desc=f\"{MODEL_ID} beam_search\"):\n",
    "    cfg = dcfg(\"beam_search\", num_beams=int(b))\n",
    "    texts = generate_microbatched(prompts_gen, cfg, microbatch=MICROBATCH)\n",
    "\n",
    "    new_lens = []\n",
    "    trigs = []\n",
    "    stops = []\n",
    "\n",
    "    for pr, tx in zip(prompts_gen, texts):\n",
    "        new_len = max(tok_len(tx) - tok_len(pr), 0)\n",
    "        new_lens.append(new_len)\n",
    "        trigs.append(distinct_trigrams(tx))\n",
    "        stops.append(new_len < int(STOP_EARLY_FRAC * MAX_NEW_TOKENS))\n",
    "\n",
    "    rows.append({\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"beams\": b,\n",
    "        \"avg_new_tokens\": float(np.mean(new_lens)),\n",
    "        \"avg_distinct_trigrams\": float(np.mean(trigs)),\n",
    "        \"stop_early_rate\": float(np.mean(stops)),\n",
    "    })\n",
    "\n",
    "df_beam = pd.DataFrame(rows)\n",
    "df_beam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95b3309",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df_beam[\"beams\"], df_beam[\"avg_new_tokens\"], marker=\"o\")\n",
    "plt.xlabel(\"beam width\")\n",
    "plt.ylabel(\"avg continuation length (tokens)\")\n",
    "plt.title(f\"{MODEL_ID}: beam width → length\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_beam[\"beams\"], df_beam[\"stop_early_rate\"], marker=\"x\", linestyle=\"--\")\n",
    "plt.xlabel(\"beam width\")\n",
    "plt.ylabel(\"stop-early rate\")\n",
    "plt.title(f\"{MODEL_ID}: beam width → stop-early\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_beam[\"beams\"], df_beam[\"avg_distinct_trigrams\"], marker=\"o\")\n",
    "plt.xlabel(\"beam width\")\n",
    "plt.ylabel(\"avg distinct word trigrams\")\n",
    "plt.title(f\"{MODEL_ID}: beam width → distinct trigrams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1e465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe = MODEL_ID.replace(\"/\", \"__\")\n",
    "df_main.to_csv(f\"results_main__{safe}.csv\", index=False)\n",
    "df_beam.to_csv(f\"results_beam__{safe}.csv\", index=False)\n",
    "\n",
    "del engine\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Saved CSVs and freed GPU memory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-project (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
